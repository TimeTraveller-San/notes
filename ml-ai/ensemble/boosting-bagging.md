# Ensembles, bagging and boosting

Alexander Ihler's youtube videos: 

- Basics of ensembles: https://www.youtube.com/watch?v=Yvn3--rIdZg
- Bagging: https://www.youtube.com/watch?v=Rm6s6gmLTdg
- Gradient boosting: https://www.youtube.com/watch?v=sRktKszFmSk
- Adaboost: https://www.youtube.com/watch?v=ix6IvwbVpw0

# Bagging trees into random forests
I wrote a blog on it which is based off fastai's bagging tree implementation:

https://towardsdatascience.com/random-forests-and-decision-trees-from-scratch-in-python-3e4fa5ae4249


# Adaboost 
Adaboost = Adaptive boosting


She explained adaboost here:
https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/



# Xgboost
Xgboost = extreme gradient boosting


xgboost is a faster and regularized implementation of gradient boosting machine reading this helped: https://xgboost.readthedocs.io/en/latest/tutorials/model.html

rather start reading from here: https://xgboost.readthedocs.io/en/latest/tutorials/model.html#decision-tree-ensembles


Tianqi Chen (creater of XGboost) wrote a series of blog articles over it, read it if you ever get time: https://homes.cs.washington.edu/~tqchen/2016/03/10/story-and-lessons-behind-the-evolution-of-xgboost.html


